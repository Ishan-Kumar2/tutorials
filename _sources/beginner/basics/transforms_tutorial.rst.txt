
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "beginner/basics/transforms_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_beginner_basics_transforms_tutorial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_beginner_basics_transforms_tutorial.py:


`Learn the Basics <intro.html>`_ ||
`Quickstart <quickstart_tutorial.html>`_ ||
`Tensors <tensorqs_tutorial.html>`_ ||
`Datasets & DataLoaders <data_tutorial.html>`_ ||
**Transforms** ||
`Build Model <buildmodel_tutorial.html>`_ ||
`Autograd <autogradqs_tutorial.html>`_ ||
`Optimization <optimization_tutorial.html>`_ ||
`Save & Load Model <saveloadrun_tutorial.html>`_

Transforms
===================

Data does not always come in its final processed form that is required for
training machine learning algorithms. We use **transforms** to perform some
manipulation of the data and make it suitable for training.

All TorchVision datasets have two parameters -``transform`` to modify the features and
``target_transform`` to modify the labels - that accept callables containing the transformation logic.
The `torchvision.transforms <https://pytorch.org/vision/stable/transforms.html>`_ module offers
several commonly-used transforms out of the box.

The FashionMNIST features are in PIL Image format, and the labels are integers.
For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors.
To make these transformations, we use ``ToTensor`` and ``Lambda``.

.. GENERATED FROM PYTHON SOURCE LINES 28-41

.. code-block:: default


    import torch
    from torchvision import datasets
    from torchvision.transforms import ToTensor, Lambda

    ds = datasets.FashionMNIST(
        root="data",
        train=True,
        download=True,
        transform=ToTensor(),
        target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz

      0%|          | 0/26421880 [00:00<?, ?it/s]
      0%|          | 65536/26421880 [00:00<01:12, 363717.32it/s]
      1%|          | 229376/26421880 [00:00<00:38, 683090.25it/s]
      3%|3         | 917504/26421880 [00:00<00:12, 2108933.28it/s]
      7%|7         | 1900544/26421880 [00:00<00:07, 3422273.25it/s]
     11%|#1        | 2949120/26421880 [00:00<00:05, 4278023.37it/s]
     16%|#5        | 4128768/26421880 [00:01<00:04, 5039958.69it/s]
     21%|##        | 5439488/26421880 [00:01<00:03, 5754262.49it/s]
     26%|##6       | 6881280/26421880 [00:01<00:03, 6458534.63it/s]
     32%|###1      | 8454144/26421880 [00:01<00:02, 7164011.49it/s]
     39%|###8      | 10223616/26421880 [00:01<00:02, 7959722.68it/s]
     46%|####6     | 12156928/26421880 [00:01<00:01, 8785395.07it/s]
     54%|#####3    | 14254080/26421880 [00:02<00:01, 9659447.64it/s]
     63%|######2   | 16613376/26421880 [00:02<00:00, 10660812.93it/s]
     73%|#######2  | 19202048/26421880 [00:02<00:00, 11752393.06it/s]
     83%|########3 | 22020096/26421880 [00:02<00:00, 12942236.30it/s]
     95%|#########5| 25165824/26421880 [00:02<00:00, 14236074.84it/s]
    100%|##########| 26421880/26421880 [00:02<00:00, 9109781.76it/s] 
    Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz

      0%|          | 0/29515 [00:00<?, ?it/s]
    100%|##########| 29515/29515 [00:00<00:00, 329238.21it/s]
    Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz

      0%|          | 0/4422102 [00:00<?, ?it/s]
      1%|          | 32768/4422102 [00:00<00:24, 181537.26it/s]
      1%|1         | 65536/4422102 [00:00<00:24, 181166.07it/s]
      2%|2         | 98304/4422102 [00:00<00:23, 181136.22it/s]
      3%|2         | 131072/4422102 [00:00<00:23, 181144.44it/s]
      4%|4         | 196608/4422102 [00:00<00:20, 208627.33it/s]
      6%|5         | 262144/4422102 [00:01<00:16, 254942.28it/s]
      7%|6         | 294912/4422102 [00:01<00:17, 232925.30it/s]
      8%|8         | 360448/4422102 [00:01<00:14, 271603.07it/s]
     10%|9         | 425984/4422102 [00:01<00:13, 298887.55it/s]
     11%|#1        | 491520/4422102 [00:01<00:12, 317977.84it/s]
     13%|#3        | 589824/4422102 [00:02<00:09, 385231.49it/s]
     15%|#4        | 655360/4422102 [00:02<00:09, 378872.49it/s]
     17%|#7        | 753664/4422102 [00:02<00:08, 427765.54it/s]
     19%|#9        | 851968/4422102 [00:02<00:07, 462884.87it/s]
     22%|##2       | 983040/4422102 [00:02<00:06, 540201.75it/s]
     25%|##5       | 1114112/4422102 [00:02<00:05, 594787.71it/s]
     28%|##8       | 1245184/4422102 [00:03<00:05, 634370.80it/s]
     31%|###1      | 1376256/4422102 [00:03<00:04, 710829.59it/s]
     33%|###3      | 1474560/4422102 [00:03<00:04, 718817.90it/s]
     37%|###7      | 1638400/4422102 [00:03<00:03, 781270.69it/s]
     41%|####1     | 1835008/4422102 [00:03<00:02, 878633.51it/s]
     47%|####6     | 2064384/4422102 [00:03<00:02, 999720.38it/s]
     52%|#####1    | 2293760/4422102 [00:04<00:01, 1083239.42it/s]
     58%|#####7    | 2555904/4422102 [00:04<00:01, 1195048.54it/s]
     64%|######4   | 2850816/4422102 [00:04<00:01, 1325736.99it/s]
     71%|#######1  | 3145728/4422102 [00:04<00:00, 1527899.97it/s]
     76%|#######5  | 3342336/4422102 [00:04<00:00, 1512266.20it/s]
     84%|########3 | 3702784/4422102 [00:04<00:00, 1672486.72it/s]
     93%|#########2| 4096000/4422102 [00:05<00:00, 1833917.66it/s]
    100%|##########| 4422102/4422102 [00:05<00:00, 841842.90it/s] 
    Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz

      0%|          | 0/5148 [00:00<?, ?it/s]
    100%|##########| 5148/5148 [00:00<00:00, 35109393.48it/s]
    Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw





.. GENERATED FROM PYTHON SOURCE LINES 42-49

ToTensor()
-------------------------------

`ToTensor <https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor>`_
converts a PIL image or NumPy ``ndarray`` into a ``FloatTensor``. and scales
the image's pixel intensity values in the range [0., 1.]


.. GENERATED FROM PYTHON SOURCE LINES 51-59

Lambda Transforms
-------------------------------

Lambda transforms apply any user-defined lambda function. Here, we define a function
to turn the integer into a one-hot encoded tensor.
It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls
`scatter_ <https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html>`_ which assigns a
``value=1`` on the index as given by the label ``y``.

.. GENERATED FROM PYTHON SOURCE LINES 59-63

.. code-block:: default


    target_transform = Lambda(lambda y: torch.zeros(
        10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))








.. GENERATED FROM PYTHON SOURCE LINES 64-66

--------------


.. GENERATED FROM PYTHON SOURCE LINES 68-71

Further Reading
~~~~~~~~~~~~~~~~~
- `torchvision.transforms API <https://pytorch.org/vision/stable/transforms.html>`_


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  10.410 seconds)


.. _sphx_glr_download_beginner_basics_transforms_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: transforms_tutorial.py <transforms_tutorial.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: transforms_tutorial.ipynb <transforms_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
